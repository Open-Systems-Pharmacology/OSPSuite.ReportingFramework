---
title: "Data import by dictionary"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{data_import_by_dictionary}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
library(ospsuite.reportingframework)
library(data.table)
library(tidyr)
```

# Introduction
Welcome to the data import vignette for the `OSPSuite.ReportingFramework` package. This vignette aims to provide a comprehensive understanding of the data import process within the package. Importing and processing observed data is a critical step in many scientific and analytical workflows. With the help of our package, users can efficiently import, process, and validate observed data sets using a customizable data dictionary and an Excel-based template.

# Using readObservedDataByDictionary Function
The `readObservedDataByDictionary` function plays a pivotal role in the data import process within our package. It is designed to read and process observed data based on the provided project configuration and the data dictionary defined in an Excel template. Here's how you can use the function and fill the Excel table for effective data import:

To use the `readObservedDataByDictionary` function, follow these steps:

- Provide Project Configuration: The function requires the project configuration data to be passed as an argument. This configuration should include the necessary information for data import, such as the data importer configuration file and project configuration directory path. (See vignette xxx)

- Data importer configuration file: Ensure that the Excel template containing the data dictionary and data file information is available and accessible to the function. The data dictionary in the Excel template defines the mapping and conversion rules for the observed data.

- Invoke the Function: Call the `readObservedDataByDictionary` function, passing the project configuration as an argument. The function will read the data files and process the observed data based on the provided dictionary and configuration. The function will return the observed data 'as.data.table' based on the dictionary, ready for further analysis.   

- Data which is used in other configuration files is also updated: 

  - Sheet "DataGroups" (plotconfiguration file): All missing combinations of studyId, group were added. Here you can add properties like the Displayname for the report.  
  - Sheet "OutputPaths" (plotconfiguration file): All missing OutputPathIds are added. Her you can add output paths and Display names for report.  
  - Sheet "IndividualBiometrics" (Individual parameter file): All biometrics contained in the data are added.  
  
- Convert the data to the dataCombined format: Call `convertDataTableToDataCombined(dataDT)` 
  

```{r readObservedDataByDictionary, eval = FALSE, echo = TRUE}
# Call the readObservedDataByDictionary function
observedData <- readObservedDataByDictionary(projectConfiguration)

```


# Filling the Excel Table

To effectively fill the Excel table for use with the `readObservedDataByDictionary` function, follow these guidelines:

## "DataFiles" Sheet: 

In the "DataFiles" sheet of the Excel template, provide the following information:

- **DataFile**: Path of the CSV file relative to the configuration XLSX.
- **Dictionary**: Sheet name for the data dictionary.
- **DataFilter**: An R executable expression that filters relevant data for the study. If empty, no filter is applied.
- **DataClass**: Differentiate between aggregated and individual data.

## "tpDictionary" Sheet: 

In the "tpDictionary" sheet, define the data dictionary with the following columns:

  - **targetColumn**: Internal column name of the package.  
  - **type**: Type of parameter used by the package. 
      following types exist: 
      
      - **identifier** all identifiers are mandatory:  
      
         - studyId: id of study
         - studyArm: study arm
         - subjectId: id of subject with the study (not needed for aggregated data)
         - individualId: unique individual id over all studies (not needed for aggregated data)
         - group: identifier for data group, is unique over all studies and data classes
         - outputPathId: identifier for the output.
         
      - **timeprofile** columns used to process time profiles.
         The columns 'xValues','yValues','yUnit','lloq' are alyways mandatory, 
         for aggregated data the columns yErrorValues, yErrorType, yMin, yMax, nBelowLLOQ are also available:
      
        - time: time values, unit is specified in dictionary
        - yValues: data value
        - yUnit: unit of data value, (is also valid for all corresponding columns like lloq, yErrorValues)
        - lloq: lower limit for quantification,  for values below lloq set yValues to lloq/2, if not available set to NA  
        - yErrorType: type of aggregation range 
          There are two defaults for `yErrorType`:
         
           - `ArithmeticStdDev` interpretes `yValues` as mean and `yErrorValues` as standard deviation
           - `GeometricStdDev` interpretes `yValues` as geomean and `yErrorValues` as geometric standard deviation
         
           For the defaults the legend is automatically generated and `yMin` and `yMax` are ignored.
           For non defaults `yErrorType` is interpreted as legend, 
           `yErrorValues`are ignored and `yMin` and `yMax` are used.
          This can be used e.g. for median and percentiles.
        - yErrorValues: value of aggregation range, 
        - yMin: lower range of aggregation range,
        - yMax: upper range of aggregation range
        - nBelowLLOQ: number of values below lloq

      - **biometrics** columns used to create individuals, can be used also for covariate analysis 
      All columns are optional, The values are transferred to the 'Individual.xlsx' for further use. 
      Available columns are:  
      
        - age: age 
        - weight: body weight 
        - height: body height 
        - gender: gender data should be coded as characters Male Female (case insensitive)
          or numeric coding  1=male 2= female 
        - population: population make sure to translate to one of the available PK-Sim Populations
        (see ospsuite::HumanPopulation)

      - **covariate** columns used for covariate analysis, this is the only column type where the name of the
      targetcolumn can be freely assigned by the user. Covariates are optional rows
      - **metadata** columns used to add information in the DataGroup sheet in the Plotconfiguration table. 
      The information is used to generate the data import for PK-Sim

  - **sourceColumn**: Name of the column in the source CSV.  
  - **sourceUnit**: Unit of the column in the source CSV.  
  - **filter**: An R executable expression that filters the source rows. Filters are executed in the order this table.    
  - **filterValue**: An R executable expression to set a value for the filtered rows.  

By filling out the Excel table with the required information, you can ensure that the `readObservedDataByDictionary` function can effectively read and process the observed data based on the provided data dictionary and configuration.  

**!!! ATTENTION, Do not use single quotes ' to capture strings. At the beginning of an excel cell single quotes will be ignored. Use double quotes ".**


# Example Sheets

```{r load-template, eval=TRUE, echo=FALSE}
templateFilePath <- system.file("templates", "templateProject", "Scripts", "ReportingFramework", "dataImportConfiguration.xlsx",
  package = "ospsuite.reportingframework", mustWork = TRUE
)

dataFilesSheet <- esqlabsR::readExcel(path = templateFilePath, sheet = "DataFiles")
tpDictionary <- esqlabsR::readExcel(path = templateFilePath, sheet = "tpDictionary")
```


## `DataFiles` Sheet

The first line of the sheet contains descriptions for the columns, the information for the data import starts at line 2:

```{r echo=FALSE,eval=TRUE}
dataFilesSheet <- rbind(
  dataFilesSheet[c(1), ],
  data.table(
    DataFile = c(
      "relative/path/to/data1.csv",
      "relative/path/to/data2.csv",
      "relative/path/to/data3.csv"
    ),
    Dictionary = c(
      "tpDictionary",
      "tpDictionary",
      "tpDictionaryAggregated"
    ),
    DataFilter = c("", "PKValue == 1", "STUD != 1234"),
    DataClass = c(DATACLASS$tpIndividual,DATACLASS$tpIndividual,DATACLASS$tpAggregated)
  )
)

knitr::kable(dataFilesSheet)
```

In the example above we want to read data from 3 source files. 
"data1" and "data2" have the same format and use the same dictionary for the import, for data3 the dictionary "tpDictionaryAggregated" is used. All used dictionaries have to be part of the "dataImportConfiguration.xlsx."

For Data1 we need all rows, for the other two we have filter defined. In Data2 we want to exclude all flagged wit a PKFLAG > 0 and In Data3 we want to exclude data from study 1234

## `tpDictionary` Sheet

```{r, echo=FALSE}
tpDictionary <- tpDictionary %>%
  dplyr::filter(!(targetColumn %in% c("yErrorValues", "yErrorType", "n_belowLLOQ")))

knitr::kable(tpDictionary)
```

This sheet is used for individual data. Th columns yErrorValues, yErrorType and n_belowLLOQ are set to NA .  

The `individualId` was constructed as concatenation of study Id and Individual id. 
As we want to do this for all rows filter was set to `TRUE` and the r expression, which does the concatenation was placed in the column `filterValue`.

The dictionary contains two rows for the target column population. With the first row all data rows where the source columnn `RACENAME` is "White" are set to "European_ICRP_2002", with the second row Asians are set to "Asian_Tanaka_1996".



 
