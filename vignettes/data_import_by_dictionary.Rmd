---
title: "Data import by dictionary"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Data_import_by_dictionary}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{r setup, echo=FALSE,warning=FALSE,error=FALSE,message=FALSE}
library(ospsuite.reportingframework)
library(data.table)
library(tidyr)
```

# Introduction

Welcome to the data import vignette for the OSPSuite.ReportingFramework package. This vignette aims to provide a comprehensive understanding of the data import process within the package. Importing and processing observed data is a critical step in many scientific and analytical workflows. With the help of our package, users can efficiently import observed datasets using a configurable data dictionary and an Excel-based template.


# Data Formats for Plot Functions

All plot functions in the package workflow accept observed data in two formats: the `data.table` format generated by the `readObservedDataByDictionary` function and the `DataCombined` class format from the `ospsuite-R` package. Understanding how to work with these formats is essential for effective data visualization and analysis.

1. Data.table Format

The `data.table` format is a highly efficient data structure used for storing and manipulating large datasets in R. When using the `readObservedDataByDictionary` function, the observed data is returned as a `data.table`. This format is particularly useful for performing operations such as filtering, aggregating, and merging datasets.

2. DataCombined Class Format
The DataCombined class is a specific format used by the ospsuite-R package to represent combined datasets, typically used for pharmacokinetic analyses. This format allows for more structured data management.

## Converting DataCombined to Data.table
If you have a DataCombined object and need to convert it back to a data.table, you can use the convertDataCombinedToDataTable function. This allows for flexibility in data handling, enabling users to switch between formats as needed.

```{r,echo=TRUE,eval=FALSE}
# Convert DataCombined back to data.table format
dataDT <- convertDataCombinedToDataTable(dataCombined)
```

## Converting Data.table to DataCombined
To convert a data.table to a DataCombined object, you can use the convertDataTableToDataCombined function. This function validates the observed data and organizes it into a structure suitable for pharmacokinetic analysis.

```{r echo=TRUE,eval=FALSE}
# Convert data.table to DataCombined format
dataCombined <- convertDataTableToDataCombined(observedData)
```


# Using readObservedDataByDictionary Function

The readObservedDataByDictionary function plays a pivotal role in the data import process within our package. It is designed to read and process observed data based on the provided project configuration and the data dictionary defined in an Excel template. Here's how to use the function and fill the Excel table for effective data import:

1. Provide Project Configuration: The function requires the project configuration data to be passed as an argument. This configuration should include the necessary information for data import, such as the data importer configuration file and project configuration directory path.

2. Data Importer Configuration File: Ensure that the Excel template containing the data dictionary and data file information is available and accessible to the function. The data dictionary in the Excel template defines the mapping and conversion rules for the observed data.

3. Invoke the Function: Call the readObservedDataByDictionary function, passing the project configuration as an argument. The function will read the data files and process the observed data based on the provided dictionary and configuration. It will return the observed data as a data.table, ready for further analysis.

Data Updates in Other Configuration Files: The function also updates data used in other configuration files:

- Sheet "DataGroups" (Plot Configuration File): Missing rows for the group identifier are added. Here, you can add properties like the display name for the report.
- Sheet "OutputPaths" (Plot Configuration File): Missing rows for the outputPathIds identifier are added. Here, you can add output paths and display names for the report.
 -Sheet "IndividualBiometrics" (Individual Parameter File): All individual biometrics contained in the data are added.
  

```{r readObservedDataByDictionary, eval = FALSE, echo = TRUE}
# Call the readObservedDataByDictionary function
observedData <- readObservedDataByDictionary(projectConfiguration)
```


# Filling the Excel Table

To effectively fill the Excel table for use with the `readObservedDataByDictionary` function, follow these guidelines:

## "DataFiles" Sheet: 

In the "DataFiles" sheet of the Excel template, provide the following information:

- **DataFile**: Path of the CSV file relative to the configuration XLSX.
- **Dictionary**: Sheet name for the data dictionary.
- **DataFilter**: An R executable expression that filters relevant data for the study. If empty, no filter is applied.
- **DataClass**: Differentiate between aggregated and individual data.

## "tpDictionary" Sheet: 

In the "tpDictionary" sheet, define the data dictionary with the following columns:

  - **targetColumn**: Internal column name of the package.  
  - **type**: Type of parameter used by the package. 
      following types exist: 
      
      - **identifier** all identifiers are mandatory:  
      
         - studyId: id of study
         - studyArm: study arm
         - subjectId: id of subject with the study (not needed for aggregated data)
         - individualId: unique individual id over all studies (not needed for aggregated data)
         - group: identifier for data group, is unique over all studies and data classes
         - outputPathId: identifier for the output.
         
      - **timeprofile** columns used to process time profiles.
         The columns 'xValues','yValues','yUnit','lloq' are always mandatory, 
         for aggregated data the columns 'yErrorValues', 'yErrorType', 'yMin', 'yMax', 'numberOfIndividuals' and 'nBelowLLOQ' are also available:
      
        - time: time values, unit is specified in dictionary
        - yValues: data value
        - yUnit: unit of data value, (is also valid for all corresponding columns like lloq, yErrorValues)
        - lloq: lower limit for quantification,  for values below lloq set yValues to lloq/2, if not available set to NA  
        - yErrorType: type of aggregation range 
          There are two defaults for `yErrorType`:
         
           - `ArithmeticStdDev` interprets `yValues` as mean and `yErrorValues` as standard deviation
           - `GeometricStdDev` interprets `yValues` as geomean and `yErrorValues` as geometric standard deviation
         
           For the defaults the legend is automatically generated and `yMin` and `yMax` are ignored.
           For non defaults `yErrorType` is interpreted as legend, it should contain the description of the man and the range, separated by a "|"
           `yErrorValues`are ignored and `yMin` and `yMax` are used.
          This can be used e.g. for median and percentiles.
        - yErrorValues: value of aggregation range, 
        - yMin: lower range of aggregation range,
        - yMax: upper range of aggregation range
        - nBelowLLOQ: number of values below lloq
        - numberOfIndividuals: number of values

      - **biometrics** columns used to create individuals, can be used also for covariate analysis 
      All columns are optional, The values are transferred to the 'Individual.xlsx' for further use. 
      Available columns are:  
      
        - age: age 
        - weight: body weight 
        - height: body height 
        - gender: gender data should be coded as characters Male Female (case insensitive)
          or numeric coding  1=male 2= female 
        - population: population make sure to translate to one of the available PK-Sim Populations
        (see `ospsuite::HumanPopulation`)

      - **covariate** columns used for covariate analysis, this is the only column type where the name of the
      targetcolumn can be freely assigned by the user. Covariates are optional rows
      - **metadata** columns used to add information in the DataGroup sheet in the plot configuration table. 
      The information is used to generate the data import for PK-Sim

  - **sourceColumn**: Name of the column in the source CSV.  
  - **sourceUnit**: Unit of the column in the source CSV.  
  - **filter**: An R executable expression that filters the source rows. Filters are executed in the order this table.    
  - **filterValue**: An R executable expression to set a value for the filtered rows.  

By filling out the Excel table with the required information, you can ensure that the `readObservedDataByDictionary` function can effectively read and process the observed data based on the provided data dictionary and configuration.  

**!!! ATTENTION, Do not use single quotes ' to capture strings. At the beginning of an excel cell single quotes will be ignored. Use double quotes ".**


# Example Sheets

```{r load-template, eval=TRUE, echo=FALSE}
templateFilePath <- system.file("templates", "templateProject", "Scripts", "ReportingFramework", "DataImportConfiguration.xlsx",
  package = "ospsuite.reportingframework", mustWork = TRUE
)

dataFilesSheet <-
  xlsxReadData(
    wb = templateFilePath,
    sheetName = "DataFiles",
    skipDescriptionRow = TRUE,
    emptyAsNA = FALSE,
    convertHeaders = FALSE
  )
tpDictionary <-
  xlsxReadData(
    wb = templateFilePath,
    sheetName = "tpDictionary",
    skipDescriptionRow = TRUE,
    emptyAsNA = FALSE,
    convertHeaders = FALSE
  )

```


## `DataFiles` Sheet

The first line of the sheet contains descriptions for the columns, the information for the data import starts at line 2:

```{r echo=FALSE,eval=TRUE}
dataFilesSheet <- rbind(
  dataFilesSheet[FALSE, ],
  data.table(
    DataFile = c(
      "relative/path/to/data1.csv",
      "relative/path/to/data2.csv",
      "relative/path/to/data3.csv"
    ),
    Dictionary = c(
      "tpDictionary",
      "tpDictionary",
      "tpDictionaryAggregated"
    ),
    DataFilter = c("", "PKValue == 1", "STUD != 1234"),
    DataClass = c(DATACLASS$tpIndividual, DATACLASS$tpIndividual, DATACLASS$tpAggregated)
  )
)

knitr::kable(dataFilesSheet)
```

In the example above we want to read data from 3 source files. 
"data1" and "data2" have the same format and use the same dictionary for the import, for data3 the dictionary "tpDictionaryAggregated" is used. All used dictionaries have to be part of the "dataImportConfiguration.xlsx."

For Data1 we need all rows, for the other two we have filter defined. In Data2 we want to exclude all flagged wit a `PKFLAG > 0` and In Data3 we want to exclude data from study 1234

## `tpDictionary` Sheet

```{r, echo=FALSE}
tpDictionaryInd <- tpDictionary %>%
  dplyr::filter(!(TargetColumn %in% c("yErrorValues", "yErrorType", "nBelowLLOQ", "numberOfIndividuals", "yMin", "yMax", "route")))

tpDictionaryInd[TargetColumn == "group"]$FilterValue <- 'paste(STUD,GRPNAME,"individual",sep = "_")'
tpDictionaryInd[TargetColumn == "dose"]$SourceColumn <- "DOSE"
tpDictionaryInd[TargetColumn == "dose"]$Filter <- NA

knitr::kable(tpDictionaryInd)
```

This sheet is used for individual data.   

The `individualId` was constructed as concatenation of study Id and Individual id. 
As we want to do this for all rows filter was set to `TRUE` and the r expression, which does the concatenation was placed in the column `filterValue`.

The dictionary contains two rows for the target column population. With the first row all data rows where the source column `RACENAME` is "White" are set to "European_ICRP_2002", with the second row Asians are set to "Asian_Tanaka_1996".

The data contains the covariate country in the column "COUNTRY"
Also the metadata dose is available.



 
